# Multiple Instance Learning (MIL) Framework with Neptune Logging

This framework provides a comprehensive solution for training and evaluating Multiple Instance Learning (MIL) models on patch-based data, with integrated Neptune.ai logging for experiment tracking.

## Features

- **Multiple MIL architectures**:
  - Transformer-based MIL
  - LSTM-based MIL
  - Convolutional MIL
  - Lightweight Convolutional MIL
- **Comprehensive data handling**:
  - Automatic caching for faster loading
  - Support for pre-split datasets
  - Configurable data preprocessing
- **Robust evaluation**:
  - Bootstrap confidence intervals
  - Cross-validation support
  - Multiple evaluation metrics
- **Visualization tools**:
  - Training curves
  - Confusion matrices
  - ROC and PR curves
  - Attention visualization
- **Neptune.ai integration**:
  - Experiment tracking
  - Metric logging
  - Visualization logging
  - Model artifact tracking

## Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/Beshoy22/MIL-radiomics.git
   cd mil-framework
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure Neptune logging (optional):
   - Create a `.env` file in the project root with your Neptune API key and project name:
   ```
   NEPTUNE_API_KEY=your_api_key_here
   NEPTUNE_PROJECT=your_workspace/your_project_name
   ```

## Usage

### Basic Training

To train a model with default parameters:

```bash
python main.py --data_dir /path/to/data --model_type transformer --use_neptune
```

### Cross-Validation

To train with cross-validation:

```bash
python main.py --data_dir /path/to/data --model_type transformer --cv_folds 5 --use_neptune
```

### Model Types

Choose from different MIL architectures:

```bash
# Transformer-based MIL
python main.py --data_dir /path/to/data --model_type transformer --use_neptune

# LSTM-based MIL
python main.py --data_dir /path/to/data --model_type lstm --use_neptune

# Convolutional MIL
python main.py --data_dir /path/to/data --model_type conv --use_neptune

# Lightweight Convolutional MIL
python main.py --data_dir /path/to/data --model_type lightweight_conv --use_neptune
```

### Key Arguments

- `--data_dir`: Path to the directory containing data files
- `--model_type`: Type of model architecture (`transformer`, `lstm`, `conv`, or `lightweight_conv`)
- `--endpoint`: Endpoint to use for classification (`OS_6` or `OS_24`)
- `--cv_folds`: Number of folds for cross-validation (default: 1, meaning no cross-validation)
- `--use_neptune`: Enable Neptune.ai logging
- `--output_dir`: Directory to save outputs (default: `./outputs/{model_type}`)
- `--batch_size`: Batch size for training
- `--lr`: Learning rate
- `--num_epochs`: Maximum number of training epochs
- `--patience`: Patience for early stopping
- `--seed`: Random seed for reproducibility

See all available options:

```bash
python main.py --help
```

## Data Format

The framework expects data in Python pickle (.pkl) files with the following structure:

- Each .pkl file contains a list of instances
- Each instance is a dictionary containing:
  - `features`: Patch embeddings (tensor or array) of shape [n_patches, feature_dim]
  - `OS_6` or `OS_24`: Binary label (0 or 1) for the endpoint
  - Other metadata (optional)

# Interpreting Results and Outputs

This section explains how to interpret the results generated by the MIL framework, including output files, evaluation metrics, and visualizations.

## Output Directory Structure

When you run the framework, it creates an output directory with the following structure:

```
outputs/
    ├── {model_type}/                # Standard training output
    │   ├── model.pt                 # Trained model weights
    │   ├── metrics.json             # Evaluation metrics with confidence intervals
    │   ├── history.json             # Training history
    │   ├── center_metrics.json      # Center-specific evaluation metrics (if available)
    │   ├── training_curves.png      # Training curves plot
    │   ├── confusion_matrix.png     # Confusion matrix plot
    │   ├── roc_curve.png            # ROC curve plot
    │   ├── metrics_with_ci.png      # Metrics with confidence intervals
    │   ├── dataset_comparison.png   # Comparison of metrics across datasets
    │   └── center_metrics.png       # Performance by center (if available)
    │
    └── {model_type}_cv{n}/          # Cross-validation output
        ├── best_model.pt            # Best model weights
        ├── cv_metrics.json          # Aggregated CV metrics
        ├── cv_metrics.png           # CV metrics plot
        ├── fold_1/                  # Fold 1 outputs
        ├── fold_2/                  # Fold 2 outputs
        └── ...
```

## Evaluation Metrics

The `metrics.json` file contains comprehensive evaluation metrics:

- **Accuracy**: Overall classification accuracy (higher is better)
- **Precision**: Precision score for positive class (higher is better)
- **Recall**: Recall score for positive class (higher is better)
- **F1**: F1 score for positive class (higher is better)
- **F1 Macro**: Average F1 score across all classes (higher is better)
- **F1 Weighted**: Class-weighted F1 score (higher is better)
- **AUC**: Area Under the ROC Curve (higher is better)

Each metric includes its confidence interval (`_ci`), which represents the statistical uncertainty of the result.

## Training History

The `history.json` file tracks training progress:

- **train_loss/val_loss**: Loss values for each epoch
- **train_acc/val_acc**: Accuracy values for each epoch
- **val_f1_macro/val_f1_weighted**: F1 scores for validation set

A good model should show decreasing loss and increasing accuracy/F1 scores over time, with validation metrics closely tracking training metrics (without large gaps indicating overfitting).

## Visualizations

### Training Curves

This plot shows how loss and metrics evolve during training:
- **Loss curves**: Should decrease and eventually plateau
- **Accuracy curves**: Should increase and eventually plateau
- **F1 curves**: Should increase and eventually plateau

### Confusion Matrix

### ROC Curve

### Metrics with Confidence Intervals

### Dataset Comparison

Compares model performance across training, validation, and test sets: similar performance across all sets suggests good generalization

## Center-Based Evaluation

The `center_metrics.json` file and `center_metrics.png` visualization provide performance breakdowns by center:

- **Sample counts**: Number of samples from each center
- **Performance metrics**: Metrics calculated for each center
- **Weighted averages**: Overall metrics weighted by sample count

Analyze this data to:
- Identify centers where the model performs differently
- Detect potential data distribution issues
- Assess model generalization across different data sources

## Neptune.ai Integration

Neptune.ai is used for experiment tracking and visualization. When enabled with `--use_neptune`, the framework will log:

- Model parameters and hyperparameters
- Training and validation metrics (loss, accuracy, F1 scores)
- Learning rate changes
- Best model checkpoint
- Evaluation metrics with confidence intervals
- Visualizations (training curves, confusion matrices, ROC curves)
- Cross-validation results

Access the Neptune dashboard to view and compare experiments.

# Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
